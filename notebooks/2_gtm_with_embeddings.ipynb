{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a6ddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches: 28731\n"
     ]
    }
   ],
   "source": [
    "# Load a random sample of speeches pronounced in the floor of the US Congress between 1994 and 2024\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/us_congress_speeches_sample.csv')\n",
    "\n",
    "print(\"Number of speeches: {}\".format(len(df)))\n",
    "\n",
    "# Path setup\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from corpus import Corpus\n",
    "from models import GTM\n",
    "from utils import bert_embeddings_from_list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # to avoid some warnings\n",
    "\n",
    "def embed_fn_bert(texts):\n",
    "    return bert_embeddings_from_list(\n",
    "        texts=texts,\n",
    "        sbert_model_to_load=\"paraphrase-multilingual-mpnet-base-v2\",\n",
    "        batch_size=8,\n",
    "        max_seq_length=128,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "# Create vectorizer\n",
    "default_vectorizer = CountVectorizer()\n",
    "\n",
    "# Define modalities\n",
    "modalities = {\n",
    "    \"text\": {\n",
    "        \"column\": \"doc_clean\",\n",
    "        \"views\": {\n",
    "            \"embedding\": {\n",
    "                \"type\": \"embedding\",\n",
    "                \"embed_fn\": embed_fn_bert\n",
    "            },\n",
    "            \"bow\": {\n",
    "                \"type\": \"bow\",\n",
    "                \"vectorizer\": default_vectorizer\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Corpus(df, modalities=modalities)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83dfd349-b99f-431d-a2ef-a3b9109f2318",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('train_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "# Load\n",
    "with open('train_dataset.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba19f133-be49-4f9c-bb64-ff475dc7cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_args = {\n",
    "    \"text_embedding\": {\n",
    "        \"hidden_dims\": [256,64],\n",
    "        \"activation\": \"relu\",\n",
    "        \"bias\": True,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "decoder_args = {\n",
    "    \"text_bow\": {\n",
    "        \"hidden_dims\": [64,256],\n",
    "        \"activation\": \"relu\",\n",
    "        \"bias\": True,\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85f8ea0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch   1\tMean Training Loss:8.7709431\n",
      "\n",
      "\n",
      "Epoch   2\tMean Training Loss:8.5055890\n",
      "\n",
      "\n",
      "Epoch   3\tMean Training Loss:8.3729568\n",
      "\n",
      "\n",
      "Epoch   4\tMean Training Loss:8.2937131\n",
      "\n",
      "\n",
      "Epoch   5\tMean Training Loss:8.1979330\n",
      "\n",
      "\n",
      "Epoch   6\tMean Training Loss:8.1677784\n",
      "\n",
      "\n",
      "Epoch   7\tMean Training Loss:8.1438777\n",
      "\n",
      "\n",
      "Epoch   8\tMean Training Loss:8.1272968\n",
      "\n",
      "\n",
      "Epoch   9\tMean Training Loss:8.1174837\n",
      "\n",
      "\n",
      "Epoch  10\tMean Training Loss:8.0993053\n",
      "\n",
      "\n",
      "Epoch  11\tMean Training Loss:8.0860619\n",
      "\n",
      "\n",
      "Epoch  12\tMean Training Loss:8.0751244\n",
      "\n",
      "\n",
      "Epoch  13\tMean Training Loss:8.0665987\n",
      "\n",
      "\n",
      "Epoch  14\tMean Training Loss:8.0561958\n",
      "\n",
      "\n",
      "Epoch  15\tMean Training Loss:8.0527145\n",
      "\n",
      "\n",
      "Epoch  16\tMean Training Loss:8.0473793\n",
      "\n",
      "\n",
      "Epoch  17\tMean Training Loss:8.0375476\n",
      "\n",
      "\n",
      "Epoch  18\tMean Training Loss:8.0334343\n",
      "\n",
      "\n",
      "Epoch  19\tMean Training Loss:8.0243886\n",
      "\n",
      "\n",
      "Epoch  20\tMean Training Loss:8.0181381\n",
      "\n",
      "\n",
      "Epoch  21\tMean Training Loss:8.0110870\n",
      "\n",
      "\n",
      "Epoch  22\tMean Training Loss:8.0065778\n",
      "\n",
      "\n",
      "Epoch  23\tMean Training Loss:7.9978029\n",
      "\n",
      "\n",
      "Epoch  24\tMean Training Loss:7.9946293\n",
      "\n",
      "\n",
      "Epoch  25\tMean Training Loss:7.9936611\n",
      "\n",
      "\n",
      "Epoch  26\tMean Training Loss:7.9850558\n",
      "\n",
      "\n",
      "Epoch  27\tMean Training Loss:7.9781413\n",
      "\n",
      "\n",
      "Epoch  28\tMean Training Loss:7.9747599\n",
      "\n",
      "\n",
      "Epoch  29\tMean Training Loss:7.9701536\n",
      "\n",
      "\n",
      "Epoch  30\tMean Training Loss:7.9662026\n",
      "\n",
      "\n",
      "Epoch  31\tMean Training Loss:7.9617229\n",
      "\n",
      "\n",
      "Epoch  32\tMean Training Loss:7.9581328\n",
      "\n",
      "\n",
      "Epoch  33\tMean Training Loss:7.9559675\n",
      "\n",
      "\n",
      "Epoch  34\tMean Training Loss:7.9554888\n",
      "\n",
      "\n",
      "Epoch  35\tMean Training Loss:7.9502390\n",
      "\n",
      "\n",
      "Epoch  36\tMean Training Loss:7.9486657\n",
      "\n",
      "\n",
      "Epoch  37\tMean Training Loss:7.9424938\n",
      "\n",
      "\n",
      "Epoch  38\tMean Training Loss:7.9443750\n",
      "\n",
      "\n",
      "Stopping at Epoch 38. Reverting to Epoch 37\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "tm = GTM(\n",
    "    train_data=train_dataset,\n",
    "    n_topics=20,\n",
    "    encoder_args=encoder_args,\n",
    "    decoder_args=decoder_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2e2704-5639-4ea7-be0c-ca3cde28c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic_0: ['cancer', 'health', 'care', 'child', 'patient']\n",
      "Topic_1: ['budget', 'go', 'cut', 'spending', 'debt']\n",
      "Topic_2: ['service', 'community', 'honor', 'recognize', 'life']\n",
      "Topic_3: ['team', 'win', 'championship', 'game', 'season']\n",
      "Topic_4: ['nuclear', 'country', 'weapon', 'other', 'world']\n",
      "Topic_5: ['human', 'right', 'resolution', 'peace', 'other']\n",
      "Topic_6: ['service', 'family', 'life', 'honor', 'man']\n",
      "Topic_7: ['energy', 'oil', 'fuel', 'gas', 'bill']\n",
      "Topic_8: ['tax', 'health', 'cut', 'go', 'family']\n",
      "Topic_9: ['know', 'go', 'friend', 'good', 'get']\n",
      "Topic_10: ['such', 'bill', 'subcommittee', 'section', 'include']\n",
      "Topic_11: ['school', 'student', 'education', 'program', 'teacher']\n",
      "Topic_12: ['budget', 'go', 'bill', 'spending', 'debt']\n",
      "Topic_13: ['go', 'tax', 'job', 'country', 'get']\n",
      "Topic_14: ['bill', 'law', 'legislation', 'other', 'use']\n",
      "Topic_15: ['bill', 'business', 'follow', 'leader', 'no']\n",
      "Topic_16: ['go', 'judge', 'law', 'election', 'other']\n",
      "Topic_17: ['bill', 'business', 'company', 'go', 'insurance']\n",
      "Topic_18: ['community', 'provide', 'area', 'land', 'public']\n",
      "Topic_19: ['energy', 'bill', 'oil', 'gas', 'use']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            \"{}: {}\".format(str(k), str(v))\n",
    "            for k, v in tm.get_topic_words(topK=5).items()\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ec70899-e68b-4a54-83f8-99f5b3232340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e5e5d0a806411b9941ea655712dbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: L'arme nucléaire est un atout géopolitique indéniable.\n",
      "→ Top Topic 4: nuclear, country, weapon, other, world\n",
      "\n",
      "Sentence 2: Cette équipe de football est incroyable.\n",
      "→ Top Topic 3: team, win, championship, game, season\n",
      "\n",
      "Sentence 3: Une nouvelle taxe sur la consommation entrera en vigueur dès janvier.\n",
      "→ Top Topic 8: tax, health, cut, go, family\n",
      "\n",
      "Sentence 4: Il faut réformer l'école publique avant que ce ne soit trop tard.\n",
      "→ Top Topic 11: school, student, education, program, teacher\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use multilingual embeddings as input (for encoding) and decode Bag of Words matrices.\n",
    "# This means the model can also predict topic shares out of sample for other languages (e.g., French).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the list of French sentences\n",
    "sentences = [\n",
    "    \"L'arme nucléaire est un atout géopolitique indéniable.\",\n",
    "    \"Cette équipe de football est incroyable.\",\n",
    "    \"Une nouvelle taxe sur la consommation entrera en vigueur dès janvier.\",\n",
    "    \"Il faut réformer l'école publique avant que ce ne soit trop tard.\"\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df2 = pd.DataFrame({'speech': sentences})\n",
    "\n",
    "modalities = {\n",
    "    \"text\": {\n",
    "        \"column\": \"speech\",\n",
    "        \"views\": {\n",
    "            \"embedding\": {\n",
    "                \"type\": \"embedding\",\n",
    "                \"embed_fn\": embed_fn_bert\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "french_dataset = Corpus(df2, modalities=modalities)\n",
    "\n",
    "the_topic_shares = tm.get_doc_topic_distribution(french_dataset)\n",
    "\n",
    "# Get the top topic per document\n",
    "top_topics = np.argmax(the_topic_shares, axis=1)\n",
    "\n",
    "# Get topic words dictionary\n",
    "topic_words = tm.get_topic_words(topK=5)\n",
    "\n",
    "# Format and print the results\n",
    "for i, (sentence, topic_id) in enumerate(zip(sentences, top_topics)):\n",
    "    topic_id = int(topic_id)  # Cast to plain Python int\n",
    "    words = \", \".join(topic_words[\"Topic_{}\".format(topic_id)])\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "    print(f\"→ Top Topic {topic_id}: {words}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
